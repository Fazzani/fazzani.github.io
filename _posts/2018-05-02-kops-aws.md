---
title: K8S installation with KOPS
categories: [kubernetes, aws, kops]
category: kubernetes
last_modified_at: 2018-05-02 22:10:01 0100
---
# K8S installation with KOPS

## Overview

### Kubeadm & Kops for production grade clusters

Both kubeadm and kops are good choices if you want to get a production grade cluster running in AWS fast.  The main difference between the two is whether you would rather provision your infrastructure yourself or have it done for you.

kubeadm installs clusters on existing infrastructure; whereas, kops builds the EC2 instances for you, and can also build VPC, IAM, Security groups and a number of other features as well. If you need HA masters or manifest-based cluster management, then kops may also be your first choice. But if you would rather have more control over your infrastructure, and are able to provide compatible infra for Kubernetes, then kubeadm may be a better option for you. Both are excellent production grade installers, but have different use cases.

![Kops & Cloud provider archi](https://images.contentstack.io/v3/assets/blt300387d93dabf50e/blt7b2babc7d6f895b9/5a301d7ad69e14e528c779da/download)

![Classic Aws Archi](https://github.com/aws-samples/ecs-refarch-cloudformation/raw/master/images/architecture-overview.png)

[source][overview_source]

## Install

[For more details][for_more_detail_install_k8s]

1. Kubectl Installation

   ```sh
   apt-get update && apt-get install -y apt-transport-https
   curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
   cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
   deb http://apt.kubernetes.io/ kubernetes-xenial main
   EOF
   apt-get update
   apt-get install -y kubectl
   ```

2. Kops installation

   ```sh
   wget https://github.com/kubernetes/kops/releases/download/1.9.0/kops-linux-amd64
   chmod +x kops-linux-amd64
   mv kops-linux-amd64 /usr/local/bin/kops
   ```

3. Aws cli install & configure

   1. Installation

      ```sh
      # pip install
      pip -V || sudo apt-get install python-pip
      # aws cli install
      aws --version || pip install awscli --upgrade --user
      # Add Aws to the path
      export PATH=~/.local/bin:$PATH
      ```

   2. Configuration

      ```sh
      aws configure
      AWS Access Key ID [None]: xxxxxxxxxxxxxxxxxxx
      AWS Secret Access Key [None]: xxxxxxxxxxxxxxxxxxxx
      Default region name [None]: eu-west-1
      Default output format [None]: json
      # export default AWS_PROFILE
      export AWS_PROFILE=default
      ```

   >aws autocomplete install

   ```sh
   complete -C '$(which aws_completer)' aws
   ```

4. Create a route53 domain for your cluster
5. Create an S3 bucket to store your clusters state

   S3 bucket used to store all cluster configuration information

   ```sh
   # Configurer les zones de dispo de KOPS :
   export AWS_AVAILABILITY_ZONES="$(aws ec2 describe-availability-zones --query 'AvailabilityZones[]Name' --output text | awk -v OFS="," '$1=$1')"
   # Create the S3 bucket using
   export S3_BUCKET=kops-state-store-$(cat /dev/urandom | LC_ALL=C tr -dc "[:alpha:]" | tr 'er:]' '[:lower:]' | head -c 32)
   export KOPS_STATE_STORE=s3://${S3_BUCKET}
   # S3 bucket creation
   aws s3 mb $KOPS_STATE_STORE
   # Activate S3 versioning
   aws s3api put-bucket-versioning --bucket $S3_BUCKET --versioning-configuration Status=Enabled
   ```

6. Build your cluster configuration

   ```sh
   # Créer un cluster dans un vpc privé
   kops create cluster --name test.cluster.k8s.local --master-count 3 --node-count 5 --zones    ILABILITY_ZONES --topology private --networking kube-router
   # Lister les clusters existants
   kops get cluster
   # Editer un cluster
   kops edit cluster test.cluster.k8s.local
   kops edit ig --name=test.cluster.k8s.local nodes
   kops edit ig --name=test.cluster.k8s.local master-eu-west-1a
   ```

7. Create the cluster in AWS

   ```sh
   # Mettre a jour le cluster :
   kops update cluster test.cluster.k8s.local --yes
   # Valider l'état du cluster :
   kops validate cluster
   # Récupérer le DNS du load balancer d'api :
   aws elb describe-load-balancers --query 'LoadBalancerDescriptions[*].DNSName'
   ```

## configmaps-and-secrets

### Configuration data and Secrets using AWS Parameter Store

Amazon EC2 Systems Manager eases the configuration and management of Amazon EC2 instances and associated resources.
One of the features of Systems Manager is Parameter Store that provides a centralized location to store, provide access control, and easily reference your configuration data, 
whether plain-text data such as database strings or secrets such as passwords, encrypted through AWS Key Management Service (KMS).

KMS helps you encrypt your sensitive information and protect the security of your keys. Additionally, all calls to the parameter store are recorded with AWS CloudTrail so that they can be audited.
Access to each parameter store secrets can be scoped with IAM.

Parameter Store allows three types of configuration data to be stored:

* String

* List of string

* Secure string

AWS Systems Manager constitue également un magasin centralisé permettant la gestion des données de configuration, qu'il s'agisse de données en texte brut (comme des chaînes de base de données) ou de codes secrets (comme des mots de passe). Vous pouvez ainsi séparer les codes secrets et les données de configuration du code. Les paramètres peuvent être balisés et hiérarchisés, ce qui vous aide à les gérer plus facilement. Vous pouvez par exemple utiliser le même nom de paramètre, « db-string », avec un chemin hiérarchique différent, comme « dev/dbstring » ou « prod/db-string », pour stocker différentes valeurs. Systems Manager est équipé du AWS Key Management Service (KMS), qui vous permet de chiffrer automatiquement les données que vous stockez. Vous pouvez également contrôler l'accès des utilisateurs et des ressources aux paramètres avec AWS Identity and Access Management (IAM). Les paramètres peuvent être référencés par d'autres services AWS, tels qu'Amazon Elastic Container Service, AWS Lambda et AWS CloudFormation.

```sh
# Create a secure string
aws ssm put-parameter \
  --name GREETING \
  --value Hello \
  --type SecureString \
  --key-id arn:aws:kms:eu-west-1:xxxxx:key/d5a79837-7379-4147-8c17-5cd474d2eb3c

# Get the value of the created secret
aws ssm get-parameter --name GREETING

# Decrypted value of the secret can be obtained
aws ssm get-parameter --name GREETING --with-decryption
```

### Authentication and Authorization with Kubernetes

>Kubernetes cluster created by kops has two IAM roles: a **master IAM role** and a **node IAM role**

*Kubernetes cluster created using kops has two IAM roles - one for all master nodes and one for all worker nodes.

*Please note that currently all Pods running on your cluster have access to the instance IAM role. Consider using projects such as kube2iam to prevent that.

 IAM container roles using [kube2iam][kube2iam]
    Kube2iam provides different AWS IAM roles for pods running on Kubernetes based on annotations.
    [IAM Roles To see][iam_roles]
    Permet de donner des IAM droits aux POD's (au niveau des containers)

### Admission Control for Kubernetes on AWS

>Kubernetes Admission Controllers perform semantic validation of resources during create, update, and delete operations.
Kubernetes 1.8+, you can use the **Open Policy Agent** and **Kubernetes External Admission Webhooks** to enforce custom admission control policies without recompiling or reconfiguring the Kubernetes API server.

It's for **Aws resources Admission control**. For example is used for:

* All images come from an AWS repository (other than a whitelist)
* Images are pulled from the same AWS repository in the same region as the cluster
* Each namespace is controlled by the users in different AWS IAM groups

### Defining a Network Policy

Le principe est d'isoler les networks et de mettre des restrictions sur le trafic au sein de ces réseaux. Dans Kubernetes, les networks policies sont implémentés par plugins.
Exemples de plugin network policies:

* Calico
* Weave NET

Network Policy example

```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow
spec:
  podSelector:
    matchLabels:
      app: http-echo
  ingress:
    - from:
      - podSelector:
          matchLabels:
            app: busybox
```

The rule above is stating that for every pod that has the label app: http-echo defined, allow access to it from pods that have the label app: busybox defined.

### Add Ingress Controllers to Kubernetes cluster

These controllers allow you to set rules that control the routing of external traffic to the services in your Kubernetes cluster. The role of the Ingress controller is to watch the Kubernetes API server for ingress events, and to action those events. For example, creating an Ingress resource is an event.

Kops propose deux solutions:

* Nginx Ingress Controller
* ALB Ingress Controller

### CoreDNS for Kubernetes Service Discovery

Kube-DNS default k8S DNS plugin.
CoreDNS is another CNCF project and is the successor to SkyDNS, which kube-dns is based on. It supports a number of use cases that kube-dns doesn't. As a general-purpose authoritative DNS server it has a lot of functionality that kube-dns couldn't reasonably be expected to add. [for more info][coredns_infos]

### Service mesh

The API is probably exposed to the Internet as well and you will get a lot of traffic to it. You want more control of the traffic that goes to this API. Maybe you want to support many API versions, do canary deployments, and you want to watch and keep track of each request that comes in. This is where service mesh comes into play. It doesn't matter if you want to use Linkerd, Istio, or recently announced Conduit, principals are almost the same.

Here are a few others things that service mash is capable of:

* Load balancing
* Fine-grained traffic policies
* Service discovery
* Service monitoring
* Tracing
* Routing
* Secure service to service communication

[conduit installation guide][conduit_install]

conduit dashboard : https://<k8s_api_Url>/api/v1/namespaces/conduit/services/web:http/proxy/pods

conduit grafana : https://<k8s_api_Url>/api/v1/namespaces/conduit/services/grafana:http/proxy/?refresh=5s&orgId=1

## Various

KOPS_STATE_STORE : is the source of truth for all clusters managed by Kops

1. Installer le dashboard

   ```sh
   kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
   ```

2. Récupérer le mot de passe admin

   ```sh
   kops get secrets admin --type secret -oplaintext
   ```

    [For more details][more_details_dashboard_auth]

3. Supprimer un cluster

   ```sh
   kops delete cluster test.cluster.k8s.local --yes
   ```

4. Route53

    * vip d'api : api.kube-aws.seloger.tools
    * vip d'ingress : *.k8s-aws.seloger.tools

5. kops cluster edit config

   ```sh
   kops edit cluster test.cluster.k8s.local
   # Update the cluster for changes to take effect
   kops update cluster example.cluster.k8s.local --yes
   ```

## Helm / AWS

```sh
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
kubectl edit deploy --namespace kube-system tiller-deploy
```

This will open the YAML configuration for this deployment in a text editor. Insert the line serviceAccount: tiller in the spec: template: spec section of the file:

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
automountServiceAccountToken: true
...
spec:
  replicas: 1
  selector:
    matchLabels:
      app: helm
      name: tiller
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: helm
        name: tiller
    spec:
      serviceAccount: tiller <---- ++++
      containers:
      - env:
        - name: TILLER_NAMESPACE
          value: kube-system
...
```

```sh
kubectl --namespace=kube-system edit deployment/tiller-deploy and changed automountServiceAccountToken to true.
```

---

## Glossary

* **Mesh**: The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there’s no task running on the node. The routing mesh routes all incoming requests to published ports on available nodes to an active container.
* **Gossip**: 
* **VPC**: Un cloud privé virtuel (VPC) est un réseau virtuel dédié à votre compte AWS. Il est logiquement isolé des autres réseaux virtuels dans le cloud AWS. Vous pouvez lancer vos ressources AWS, comme des instances Amazon EC2, dans votre VPC.
* **CIDR**: Classless Inter-Domain Routing. Lorsque vous créez un VPC, vous devez spécifier une plage d'adresses IPv4 pour le VPC sous la forme d'un bloc d'adresse CIDR (Classless Inter-Domain Routing), par exemple, 10.0.0.0/16. Il s'agit du bloc CIDR principal pour votre VPC.
* **ARN**: Amazon Resource name
* **ECR** : Aws EC2 Container Registry

---

## References

[Installing Kubernetes on AWS with kops (Kubernetes doc off)](https://kubernetes.io/docs/getting-started-guides/kops/)

[more_details_dashboard_auth]:https://github.com/kubernetes/kops/blob/master/docs/addons.md
[for_more_detail_install_k8s]:https://github.com/kubernetes/kops/blob/master/docs/cli/kops_create_cluster.md
[overview_source]:https://github.com/aws-samples/ecs-refarch-cloudformation
[iam_roles]:https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md
[kube2iam]:https://github.com/aws-samples/aws-workshop-for-kubernetes/tree/master/04-path-security-and-networking/402-authentication-and-authorization
[coredns_infos]:https://coredns.io/2017/03/01/coredns-for-kubernetes-service-discovery-take-2/
[Service_type_k8s_aws]:https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types
[conduit_install]:https://github.com/runconduit/conduit